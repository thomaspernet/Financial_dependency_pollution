{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replicate notebook to prepare ASIF panel data \n",
    "\n",
    "# Objective(s)\n",
    "\n",
    "Replicate the following notebook https://github.com/thomaspernet/SBC_pollution_China/blob/master/Data_preprocessing/00_Asif_raw_to_csv_preprocessing.md to prepare the data, and use more variables. If 2008 and 2009 donâ€™t have the varialbe we need, exclude them\n",
    "\n",
    "* Need to use:\n",
    "  * 'stata_year_1998.csv', \n",
    "  * 'stata_year_1999.csv',\n",
    "  *  'stata_year_2000.csv',\n",
    "  *  'stata_year_2001.csv',\n",
    "  * 'stata_year_2002.csv', \n",
    "  * 'stata_year_2003.csv',\n",
    "  *  'stata_year_2004.csv',\n",
    "  * 'stata_year_2005.csv', \n",
    "  *  'stata_year_2006.csv',\n",
    "  *  'stata_year_2007.csv',\n",
    "  *  'stata_year_2008.csv',\n",
    "  * 'stata_year_2009.csv'\n",
    "The data need to be saved in the S3, PREPARED folder\n",
    "\n",
    "# Metadata\n",
    "\n",
    "* Key: wbg66hgcz25127p\n",
    "* Parent key (for update parent):  \n",
    "* Notebook US Parent (i.e the one to update): \n",
    "* Child key: \n",
    "* Notebook US Child: \n",
    " * Epic: Epic 1\n",
    "* US: US 2\n",
    "* Date Begin: 11/14/2020\n",
    "* Duration Task: 0\n",
    "* Description: Prepare the ASIF panel data using the previous notebook and add all possible variables from 1998 to 2009\n",
    "* Step type: Prepare table\n",
    "* Status: Active\n",
    "* Source URL: US 02 Prepare ASIF\n",
    "* Task type: Jupyter Notebook\n",
    "* Users: Thomas Pernet\n",
    "* Watchers: Thomas Pernet\n",
    "* User Account: https://468786073381.signin.aws.amazon.com/console\n",
    "* Estimated Log points: 5\n",
    "* Task tag: #asif,#data-preparation\n",
    "* Toggl Tag: #data-preparation\n",
    "* Meetings:  \n",
    "* Presentation:  \n",
    "* Email Information:  \n",
    "  * thread: Number of threads: 0(Default 0, to avoid display email)\n",
    "  *  \n",
    "\n",
    "# Input Cloud Storage [AWS/GCP]\n",
    "\n",
    "## Table/file\n",
    "\n",
    "* Origin: \n",
    "* S3\n",
    "* Name: \n",
    "* DATA/FIRMS/RAW_DATA\n",
    "* Github: \n",
    "  * \n",
    "\n",
    "## Destination Output/Delivery\n",
    "\n",
    "### Table/file\n",
    "\n",
    "* Origin: \n",
    "* S3\n",
    "* Name:\n",
    "* DATA/FIRMS/PREPARED\n",
    "* GitHub:\n",
    "* https://github.com/thomaspernet/Financial_dependency_pollution/blob/master/01_data_preprocessing/01_prepare_tables/00_prepare_table_ASIF_from_S3.md\n",
    "* URL: \n",
    "  * chinese-data/DATA/FIRMS/PREPARED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connexion server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_glue import service_glue\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil, json\n",
    "import sidetable\n",
    "\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent.parent.parent)\n",
    "\n",
    "\n",
    "name_credential = 'XXX.csv'\n",
    "region = ''\n",
    "bucket = ''\n",
    "path_cred = \"{0}/creds/{1}\".format(parent_path, name_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False)\n",
    "glue = service_glue.connect_glue(client = client,\n",
    "                      bucket = bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare query POC\n",
    "\n",
    "Once you are satisfied by the jobs, create a new US in Coda to move the queries to the ETL. As a reminder, the POC is meant to test the queries, modify or improve it. A second notebook in the root of the folder `01_prepare_tables` is generated so that you can add the steps to the ETL. \n",
    "\n",
    "Basically, the life of a query is:\n",
    "\n",
    "1. Create a test notebook (in POC subfolder)\n",
    "    2. Move the queries to the second (in the root folder)\n",
    "        3. Loop 1 and 2 if needed\n",
    "2. In the deployment process, only queries in the root folder are used\n",
    "\n",
    "# Download data locally\n",
    "\n",
    "First of all, load the data locally. Use the function `list_all_files_with_prefix` to parse all the files in a given folder. Change the prefix to the name of the folder in which the data are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'DATA/RAW_DATA'\n",
    "LOCAL_PATH_CATALOGUE= os.path.join(str(Path(path).parent),\n",
    "                                          '00_data_catalogue'\n",
    "                                     )\n",
    "LOCAL_PATH_CONFIG_FILE = os.path.join(str(Path(path).parent),\n",
    "                                          '00_data_catalogue',\n",
    "                                          'temporary_local_data'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_download = False\n",
    "if to_download:\n",
    "    FILES_TO_UPLOAD = s3.list_all_files_with_prefix(prefix=prefix)\n",
    "    list(\n",
    "        map(\n",
    "            lambda x:\n",
    "            s3.download_file(key=x, path_local=LOCAL_PATH_CONFIG_FILE),\n",
    "            FILES_TO_UPLOAD\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table `XX`\n",
    "\n",
    "- Table name: `XX`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "The notebook file already contains code to analyse the dataset. It contains codes to count the number of observations for a given variables, for a group and a pair of group. It also has queries to provide the distribution for a single column, for a group and a pair of group. The queries are available in the key `ANALYSIS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Description\n",
    "\n",
    "During the categorical analysis, we wil count the number of observations for a given group and for a pair.\n",
    "\n",
    "**Count obs by group**\n",
    "\n",
    "- Index: primary group\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- percentage: Percentage of observation per primary group value over the total number of observations\n",
    "\n",
    "Returns the top 20 only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILENAME 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = os.path.join(LOCAL_PATH_CONFIG_FILE, os.path.split(FILES_TO_UPLOAD[0])[1])\n",
    "df_test = pd.read_csv(path_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the values fior each object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ = {'var': [],\n",
    "       'count':[],\n",
    "       'values': []}\n",
    "for v in df_test.select_dtypes(include='object').columns:\n",
    "    cat = df_test[v].nunique()\n",
    "    value_cat  = df_test[v].unique()\n",
    "    dic_['var'].append(v)\n",
    "    dic_['count'].append(cat)\n",
    "    dic_['values'].append(value_cat)\n",
    "(pd.DataFrame(dic_)\n",
    " .sort_values(by = ['count'], ascending = False)\n",
    " .set_index('var')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.concat([\n",
    "    df_test.isna().sum().sort_values().rename(\"count\"),\n",
    "    (df_test.isna().sum().sort_values()/len(df_test)).rename(\"pct\")\n",
    "    ], axis = 1\n",
    "    ).loc[lambda x: x['count']!=0]\n",
    "    .style\n",
    "    .format(\"{0:,.2%}\", subset=[\"pct\"], na_rep=\"-\")\n",
    "    .bar(subset=[\"count\"], color=\"#d65f5f\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objects in list(df_test.select_dtypes(include=[\"string\", \"object\"]).columns):\n",
    "    df_count = df_test.stb.freq([objects])\n",
    "    if df_count.shape[0] > 20:\n",
    "        df_count = df_count.iloc[:20, :]\n",
    "    display(\n",
    "        (\n",
    "            df_count.reset_index(drop=True)\n",
    "            .style\n",
    "            .format(\n",
    "                \"{0:,.2%}\", subset=[\"Percent\", \"Cumulative Percent\"], na_rep=\"-\"\n",
    "            )\n",
    "            .bar(subset=[\"Cumulative Percent\"], color=\"#d65f5f\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count obs by one key pair\n",
    "\n",
    "You need to pass the primary group in the cell below\n",
    "\n",
    "- Index: primary group\n",
    "- Columns: Secondary key -> All the categorical variables in the dataset\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- Total: Total number of observations per primary group value (sum by row)\n",
    "- percentage: Percentage of observations per primary group value over the total number of observations per primary group value (sum by row)\n",
    "\n",
    "Returns the top 20 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objects in list(df_test.select_dtypes(include=[\"string\", \"object\"]).columns):\n",
    "    if objects not in [primary_key]:\n",
    "        df_count = df_test.stb.freq([objects])\n",
    "        if df_count.shape[0] > 20:\n",
    "            df_count = df_count.iloc[:20, :]\n",
    "        display(\n",
    "            (\n",
    "                df_test.stb.freq([primary_key, objects])\n",
    "                .set_index([primary_key, objects])\n",
    "                .drop(columns=['Cumulative Count', 'Cumulative Percent'])\n",
    "                .iloc[:20, :]\n",
    "                .unstack(-1)\n",
    "                .style\n",
    "                .format(\n",
    "                    \"{0:,.2%}\", subset=[\"Percent\"], na_rep=\"-\"\n",
    "                )\n",
    "                .format(\n",
    "                    \"{0:,.2f}\", subset=[\"Count\"], na_rep=\"-\"\n",
    "                )\n",
    "                .background_gradient(\n",
    "                    cmap=sns.light_palette(\"green\", as_cmap=True), subset=(\"Count\")\n",
    "                )\n",
    "\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous description\n",
    "\n",
    "There are three possibilities to show the ditribution of a continuous variables:\n",
    "\n",
    "- Display the percentile\n",
    "- Display the percentile, with one primary key\n",
    "- Display the percentile, with one primary key, and a secondary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_test\n",
    "    .describe()\n",
    "    .style.format(\"{0:.2f}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Display the percentile, with one primary key\n",
    "\n",
    "The primary key will be passed to all the continuous variables\n",
    "\n",
    "- index: \n",
    "    - Primary group\n",
    "    - Percentile [.25, .50, .75, .95, .90] per primary group value\n",
    "- Columns: Secondary group\n",
    "- Heatmap is colored based on the row, ie darker blue indicates larger values for a given row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objects in list(df_test.select_dtypes(exclude=[\"string\", \"object\", 'boolean', 'datetime64[ns]']).columns):\n",
    "    if objects not in [primary_key]:\n",
    "        \n",
    "        print(\"\\nDistribution of {} by {}\\n\".format(objects, primary_key))\n",
    "        \n",
    "        display(\n",
    "            (\n",
    "                df_test\n",
    "                .groupby(primary_key)\n",
    "                .describe()[objects]\n",
    "                .sort_values(by='count', ascending=False)\n",
    "                .iloc[:20, :]\n",
    "                .style.format(\"{0:.2f}\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "In this section, we are going to perform:\n",
    "\n",
    "- Chi square test\n",
    "- Anova test\n",
    "\n",
    "To see if there is any dependence between the primary key, and the other variables.\n",
    "\n",
    "Each statistic is saved in the folder `statistical_analysis`\n",
    "\n",
    "### Chi square test\n",
    "\n",
    "There are two types of chi-square tests. Both use the chi-square statistic and distribution for different purposes:\n",
    "\n",
    "- A chi-square goodness of fit test determines if a sample data matches a population. For more details on this type, see: Goodness of Fit Test.\n",
    "- A chi-square test for independence compares two variables in a contingency table to see if they are related. In a more general sense, it tests to see whether distributions of categorical variables differ from each another.\n",
    "    - A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship.\n",
    "    - A very large chi square test statistic means that the data does not fit very well. In other words, there isnâ€™t a relationship\n",
    "    \n",
    "The formula for the chi-square statistic used in the chi square test is:\n",
    "\n",
    "$$\n",
    "\\chi_{c}^{2}=\\sum \\frac{\\left(O_{i}-E_{i}\\right)^{2}}{E_{i}}\n",
    "$$\n",
    "\n",
    "The subscript $c$ are the degrees of freedom. $O$ is your observed value and $E$ is your expected value.\n",
    "\n",
    "A low value for chi-square means there is a high correlation between your two sets of data. In theory, if your observed and expected values were equal (\"no difference\") then chi-square would be zero â€” an event that is unlikely to happen in real life\n",
    "\n",
    "### Anova\n",
    "\n",
    "An ANOVA test is a way to find out if survey or experiment results are significant. In other words, they help you to figure out if you need to reject the null hypothesis or accept the alternate hypothesis.\n",
    "\n",
    "Basically, youâ€™re testing groups to see if thereâ€™s a difference between them. Examples of when you might want to test different groups:\n",
    "\n",
    "- A group of psychiatric patients are trying three different therapies: counseling, medication and biofeedback. You want to see if one therapy is better than the others.\n",
    "- A manufacturer has two different processes to make light bulbs. They want to know if one process is better than the other.\n",
    "Students from different colleges take the same exam. You want to see if one college outperforms the other.\n",
    "\n",
    "Source: \n",
    "\n",
    "- [Chi-square](https://www.statisticshowto.com/probability-and-statistics/chi-square/)\n",
    "- [Anova](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/anova/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A README is automatically generated, and is available at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(\"https://github.com\", github_owner, github_owner, \"tree/master/00_data_catalogue/statistical_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, use 10% probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_tables = {}\n",
    "\n",
    "to_include_cat = []\n",
    "to_include_cont = []\n",
    "\n",
    "feat_obj = list(df_test.select_dtypes(include=['object']))\n",
    "feat_cont = list(df_test.select_dtypes(\n",
    "    exclude=[\"string\", \"object\", 'boolean', 'datetime64[ns]']))\n",
    "\n",
    "readme_chi_square_middle_1 = \"\"\"\n",
    "\n",
    "# Chi square\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "readme_anova_middle_1 = \"\"\"\n",
    "\n",
    "# Anova\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# CHI SQUARE\n",
    "\n",
    "for col in feat_obj:\n",
    "    table = pd.crosstab(df_test[primary_key],\n",
    "                        df_test[col],\n",
    "                        margins=False)\n",
    "    if table.shape[1] > 1:\n",
    "        stat, p, dof, expected = chi2_contingency(table)\n",
    "        critical = chi2.ppf(proba, dof)\n",
    "\n",
    "        if abs(stat) >= critical:\n",
    "            to_include_cat.append('PO Sub Type')\n",
    "            result = 'Dependent (reject H0)'\n",
    "            to_include_cat.append(col)\n",
    "        else:\n",
    "            result = 'Independent (fail to reject H0)'\n",
    "\n",
    "        dic_results = {\n",
    "            'test': 'Chi Square',\n",
    "            'primary_key': primary_key,\n",
    "            'secondary_key': col,\n",
    "            'statistic': stat,\n",
    "            'p_value': p,\n",
    "            'dof': dof,\n",
    "            'critical': critical,\n",
    "            'result': result\n",
    "        }\n",
    "\n",
    "        dic_tables[col] = dic_results\n",
    "\n",
    "        # Tables\n",
    "        total_obs = table.sum(axis=0).sum()\n",
    "        cont_table = (\n",
    "            table.assign(total_rows=lambda x: x.sum(axis=1))\n",
    "            .append(table.sum(axis=0).rename('total_columns'))\n",
    "            .fillna(total_obs)\n",
    "        )\n",
    "        dic_contengency = {\n",
    "\n",
    "            'contengency': cont_table.to_json(),\n",
    "            'pearson_residual': ((table - expected) / np.sqrt(expected)).to_json(),\n",
    "            'pct_row': (table.apply(lambda r: r / r.sum(), axis=1)).to_json(),\n",
    "            'pct_columns': (table.apply(lambda r: r / r.sum(), axis=0)).to_json(),\n",
    "            'pct_total': (table.apply(lambda r: r / total_obs)).to_json()\n",
    "        }\n",
    "\n",
    "        path_name = os.path.join(\n",
    "            LOCAL_PATH_CATALOGUE, \"statistical_analysis\", 'chi-square', col.replace('/', ''))\n",
    "        with open('{}.json'.format(path_name), \"w\") as outfile:\n",
    "            json.dump(dic_contengency, outfile)\n",
    "\n",
    "        if cont_table.shape[1] > 20:\n",
    "            cont_table = cont_table.iloc[:, np.r_[:10, -10:-1, -1]]\n",
    "            is_full = 'Troncated, only first/last 10 columns'\n",
    "        else:\n",
    "            is_full = 'Full table'\n",
    "        readme_chi_square_middle_2 = \"\"\"\n",
    "\n",
    "### {0}\n",
    "\n",
    "- Results between {0} and {1}: {2}\n",
    "- Contengency table ({4}):\n",
    "\n",
    "{3}\n",
    "\n",
    "        \"\"\".format(col, primary_key, result, cont_table.to_markdown(), is_full)\n",
    "\n",
    "        readme_chi_square_middle_1 += readme_chi_square_middle_2\n",
    "\n",
    "for col in feat_cont:\n",
    "    result = df_test.groupby(primary_key)[col].apply(list)\n",
    "    F, p = stats.f_oneway(*result)\n",
    "    if p <= 1 - proba:\n",
    "        result = 'Dependent (fail to reject H0)'\n",
    "        to_include_cont.append(col)\n",
    "        \n",
    "    else:\n",
    "        result = 'Independent (reject H0)'\n",
    "\n",
    "    dic_results = {\n",
    "        'test': 'Anova',\n",
    "        'primary_key': primary_key,\n",
    "        'secondary_key': col,\n",
    "        'statistic': F,\n",
    "        'p_value': p,\n",
    "        'result': result\n",
    "    }\n",
    "\n",
    "    dic_tables[col] = dic_results\n",
    "\n",
    "    readme_anova_middle_2 = \"\"\"\n",
    "    \n",
    "### {0}\n",
    "    \n",
    "- Results between {0} and {1}: {2}\n",
    "    \n",
    "    \"\"\".format(col, primary_key, result)\n",
    "\n",
    "    readme_anova_middle_1 += readme_anova_middle_2\n",
    "    \n",
    "full_table = (\n",
    "    pd.DataFrame(dic_tables).T\n",
    "    .sort_values(by = ['test', 'result'])\n",
    "    .assign(\n",
    "        statistic = lambda x: np.round(x['statistic'].astype('float'), 2),\n",
    "        dof = lambda x: np.round(x['dof'].astype('float'), 2),\n",
    "        critical = lambda x: np.round(x['critical'].astype('float'), 2),\n",
    "        p_value = lambda x: np.round(x['p_value'].astype('float'), 2),\n",
    "    )\n",
    "    #\n",
    ")\n",
    "\n",
    "readme_top = \"\"\"\n",
    "# Statistical Analysis \n",
    "\n",
    "The primary key is {0}\n",
    "\n",
    "The full results are listed below:\n",
    "\n",
    "{1}\n",
    "\n",
    "List of relevant variables:\n",
    "\n",
    "\"\"\".format(primary_key, full_table.fillna('-').to_markdown())\n",
    "\n",
    "# Save README\n",
    "to_include = to_include_cat + to_include_cont\n",
    "for i, val in enumerate(to_include):\n",
    "    relevant_var = \"{}. {}\\n\".format(i+1, val)\n",
    "    readme_top += relevant_var\n",
    "\n",
    "path_readme = os.path.join(\n",
    "    LOCAL_PATH_CATALOGUE, 'statistical_analysis', \"README.md\")\n",
    "with open(path_readme, \"w\") as outfile:\n",
    "    outfile.write(readme_top + readme_chi_square_middle_1 +\n",
    "                  readme_anova_middle_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    full_table\n",
    "    .style\n",
    "                .format(\n",
    "                    \"{0:,.2%}\", subset=[\"p_value\"], na_rep=\"-\"\n",
    "                )\n",
    "                .format(\n",
    "                    \"{0:,.2f}\", subset=[\"statistic\", \"dof\", 'critical'], na_rep=\"-\"\n",
    "                )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize in more detail the contency table, you can use the function `contengency_table`. The function parses the folder `statistical_analysis/chi-square`. Five tables are generated:\n",
    "\n",
    "- Contengency table full: contengency\n",
    "- Pearson contribution: pearson_residual\n",
    "- Centengency table percentage row-wise: pct_row\n",
    "- Centengency table percentage column-wise: pct_columns\n",
    "- Centengency table percentage full: pct_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_contengency(filename, option='contengency', style=True):\n",
    "    \"\"\"\n",
    "    Read the contengency table\n",
    "    filename: Filename to load, including `.json`. \n",
    "    Check the folder `statistical_analysis/chi-square`  to get the name\n",
    "    \"\"\"\n",
    "    path_name = os.path.join(LOCAL_PATH_CATALOGUE,\n",
    "                             \"statistical_analysis\", 'chi-square', filename)\n",
    "\n",
    "    with open(path_name, 'r') as fp:\n",
    "        table = json.load(fp)\n",
    "\n",
    "    if option in ['pct_row', 'pct_columns', 'pct_total']:\n",
    "\n",
    "        table = pd.read_json(table[option])\n",
    "\n",
    "        if style:\n",
    "            table = (\n",
    "                table\n",
    "                .style\n",
    "                .format(\n",
    "                    \"{0:,.2%}\", na_rep=\"-\"\n",
    "                )\n",
    "                .background_gradient(\n",
    "                    cmap=sns.light_palette(\"green\", as_cmap=True)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return table\n",
    "    else:\n",
    "        table = pd.read_json(table[option])\n",
    "        if style:\n",
    "            table = (table.style\n",
    "                     .background_gradient(\n",
    "                         cmap=sns.light_palette(\"green\", as_cmap=True)\n",
    "                     )\n",
    "                     )\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ''\n",
    "read_contengency(filename, option = 'pct_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "\n",
    "- Heatmap\n",
    "- Diverging bar\n",
    "- Scatter plot\n",
    "- Correspondance analysis\n",
    "\n",
    "### \n",
    "- heatmap, code by [Seaborn](https://seaborn.pydata.org/examples/many_pairwise_correlations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Generate a large random dataset\n",
    "d = df_test.select_dtypes(\n",
    "    exclude=[\"string\", \"object\", 'boolean', 'datetime64[ns]'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = d.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot\n",
    "\n",
    "From the correlation plot above, pick up a $y$ variables.\n",
    "\n",
    "We only plot the variables that succeed the Anova test, minus the $y$ var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var = ''\n",
    "for i, col in enumerate(to_include_cont):\n",
    "    if col != y_var:\n",
    "        #plt.figure(i)\n",
    "        f, ax = plt.subplots(figsize=(7, 7))\n",
    "        ax.set(xscale=\"log\", yscale=\"log\")\n",
    "        (sns.regplot(x=col, y=y_var, data=df_test, ax=ax, scatter_kws={\"s\": 100})\n",
    "         .set_title('Scatterplot between {} and {}'.format(y_var, col))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diverging bars\n",
    "\n",
    "The diverging bar plot is plotting for the variables to succeed the Anova test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col  in to_include_cont:\n",
    "    df_test_ = df_test.groupby([primary_key])[col].mean().reset_index()\n",
    "    df_test_['mean'] = (df_test_[col] - df_test_[col].mean())/df_test_[col].std()\n",
    "    df_test_['colors'] = ['red' if x < 0 else 'green' for x in df_test_['mean']]\n",
    "    df_test_.sort_values('mean', inplace=True)\n",
    "    df_test_.reset_index(inplace=True)\n",
    "    # Draw plot\n",
    "    plt.figure(figsize=(14, 10), dpi=80)\n",
    "    plt.hlines(y=df_test_.index, xmin=0, xmax=df_test_['mean'],\n",
    "               color=df_test_['colors'], alpha=0.4, linewidth=5)\n",
    "    # Decorations\n",
    "    text = \"Diverging Bars of {} within {} \".format(col, primary_key)\n",
    "    plt.gca().set(ylabel=primary_key, xlabel=col)\n",
    "    plt.yticks(df_test_.index, df_test_[primary_key], fontsize=12)\n",
    "    plt.title(text, fontdict={'size': 20})\n",
    "    plt.grid(linestyle='--', alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correspondance analysis\n",
    "\n",
    "We created a Python library to make a correspondance analysis. Please, refers to [https://github.com/thomaspernet/Correspondence_analysis](https://github.com/thomaspernet/Correspondence_analysis/blob/master/CorrespondenceAnalysisPy/correspondence_analysis_computation/ca_compute.py) for the codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CorrespondenceAnalysisPy.correspondence_analysis_computation import ca_compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in to_include_cat:\n",
    "    name = '{}.json'.format(var)\n",
    "    try:\n",
    "        tb = read_contengency(filename=name, option='contengency', style=False)\n",
    "        ca = ca_compute.compute_ca(\n",
    "            (\n",
    "                tb\n",
    "                .iloc[:-1, :-1]\n",
    "            )\n",
    "        )\n",
    "        ca_computed = ca.correspondance_analysis()\n",
    "        fig_2 = ca_compute.row_focus_coordinates(\n",
    "            df_x=ca_computed['pc_rows'],\n",
    "            df_y=ca_computed['pc_columns'],\n",
    "            variance_explained=ca_computed['variance_explained'],\n",
    "            export_data=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ca = ca_compute.compute_ca(\n",
    "(\n",
    "    read_contengency(filename = 'PO Sub Type.json', option = 'contengency', style = False)\n",
    "    .iloc[:-1, :-1]\n",
    ")\n",
    ")\n",
    "ca_computed = ca.correspondance_analysis()\n",
    "fig_2 = ca_compute.row_focus_coordinates(\n",
    "                df_x=ca_computed['pc_rows'],\n",
    "                df_y=ca_computed['pc_columns'],\n",
    "                variance_explained=ca_computed['variance_explained'],\n",
    "                export_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Generate reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "create_report(extension = \"html\", keep_code = False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.23.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
