{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK NAME FROM CODA TASK\n",
    "\n",
    "Copy paste from Coda to fill the information\n",
    "\n",
    "## Objective(s)\n",
    "\n",
    "\n",
    "## Metadata \n",
    "\n",
    "* Metadata parameters are available here: \n",
    "* Task type:\n",
    "  * \n",
    "* Users: :\n",
    "  * \n",
    "* Watchers:\n",
    "  * \n",
    "* Estimated Log points:\n",
    "  * One being a simple task, 15 a very difficult one\n",
    "  *  \n",
    "* Task tag\n",
    "  *  \n",
    "* Toggl Tag\n",
    "  * \n",
    "* Instance [AWS]\n",
    "  *   \n",
    "  \n",
    "## Input Cloud Storage [AWS]\n",
    "\n",
    "If link from the internet, save it to the cloud first\n",
    "\n",
    "### Tables [AWS]\n",
    "\n",
    "1. Batch 1:\n",
    "  * Select Provider: \n",
    "  * Select table(s): \n",
    "    * Select only tables created from the same notebook, else copy/paste selection to add new input tables\n",
    "    * If table(s) does not exist, add them: Add New Table\n",
    "    * Information:\n",
    "      * Region: \n",
    "        * Name: \n",
    "        * Code: \n",
    "      * Database: \n",
    "      * Notebook construction file: \n",
    "    \n",
    "## Destination Output/Delivery\n",
    "\n",
    "* AWS\n",
    "  1. Athena: \n",
    "      * Region: \n",
    "      * Database: \n",
    "      * Tables (Add name new table): \n",
    "\n",
    "## Things to know (Steps, Attention points or new flow of information)\n",
    "\n",
    "### Sources of information  (meeting notes, Documentation, Query, URL)\n",
    "\n",
    "\n",
    "# S3 Architecture\n",
    "\n",
    "The S3 architecture for the algorithm is as follow:\n",
    "\n",
    "```\n",
    "├── ALGORITHM\n",
    "│   ├── PYTHON_SCRIPTS\n",
    "│   └── YYYYMMDD\n",
    "│       ├── ALGO_NAME\n",
    "│       │   ├── LOGS\n",
    "│       │   └── MODEL\n",
    "│       ├── DATA\n",
    "│       │   ├── PREDICT\n",
    "│       │   │   ├── RAW\n",
    "│       │   │   └── TRANSFORM\n",
    "│       │   └── TRAIN\n",
    "│       │       ├── RAW\n",
    "│       │       └── TRANSFORM\n",
    "│       └── EVALUATION\n",
    "```\n",
    "\n",
    "where `YYYYMMDD` is the date when the model has been trained or retrained. Within this folder, there are two Childs, `ALGO_NAME` and `DATA`. The folder `ALGO_NAME` should be renamed according to the algorithm used (ie `XGBOOST`, `RNN` , etc). \n",
    "\n",
    "- `YYYYMMDD`\n",
    "  - `DATA`\n",
    "    - `TRAIN` -> contains the raw data to be trained on\n",
    "      - `RAW`: Training raw data before preprocessing\n",
    "      - `TRANSFORM`: Training data after preprocessing\n",
    "      - Sagemaker referenced functions: [SKLearnProcessor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html?highlight=SKLearnProcessor#sagemaker.sklearn.processing.SKLearnProcessor) or [ScriptProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html?highlight=ScriptProcessor#sagemaker.processing.ScriptProcessor) for preprocessing and [SKLearn](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html?highlight=SKLearn#sagemaker.sklearn.estimator.SKLearn) for training\n",
    "    - `PREDICT`: -> contains the raw data to be predicted\n",
    "      - `RAW`: Prediction raw data before preprocessing\n",
    "      - `TRANSFORM`: Prediction data after preprocessing\n",
    "      - Sagemaker referenced function: [SKLearnProcessor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html?highlight=SKLearnProcessor#sagemaker.sklearn.processing.SKLearnProcessor) or [ScriptProcessor](https://sagemaker.readthedocs.io/en/stable/api/training/processing.html?highlight=ScriptProcessor#sagemaker.processing.ScriptProcessor) for prediction\n",
    "  - `ALGO_NAME` : Name of the algorithm used to train and predict the data\n",
    "    - `LOGS`: Logs generated by Sagemaker during the training\n",
    "    - `MODEL`: `tar` file with the model's object\n",
    "  - `EVALUATION`: -> Contains the model evaluation performances\n",
    "- `PYTHON_SCRIPTS`: Contains the preprocessing, training and evaluating scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create preprocessing class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "#from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.sklearn.processing import ScriptProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "### use this code when sagemaker managed image\n",
    "#sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "#                                     role=role,\n",
    "#                                     instance_type='ml.m5.xlarge',\n",
    "#                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_repository_uri = '869881768412.dkr.ecr.eu-west-2.amazonaws.com/sagemaker-xgboost-container:latest'\n",
    "sklearn_processor = ScriptProcessor(command=['python3'],\n",
    "                image_uri=processing_repository_uri,\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo -u ec2-user -i conda install -c conda-forge jupytext --y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo -u ec2-user -i jupyter nbextension install --py jupytext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom role is availables here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "matches = re.search('([^\\/]+$)', role).group()\n",
    "url = 'https://console.aws.amazon.com/iam/home?region={0}#/roles/{1}'.format(region, matches)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our custom library to load the data from Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "\n",
    "path_cred = \"s3://creditsafedata/CREDENTIALS/thomas_credentials.csv\"\n",
    "bucket = 'creditsafedata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas_setting = True\n",
    "#if pandas_setting:\n",
    "#    pd.set_option('display.max_columns', None)\n",
    "#    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "The analysis steps performed in this notebook are the following\n",
    "\n",
    "\n",
    "- Write preprocessing pipeline\n",
    "- Load the table to S3\n",
    "- Train the model on XGBOOST\n",
    "- Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write preprocessing pipeline\n",
    "\n",
    "Write the preprocessing pipeline using Scikit learn. Please refer to the [official AWS tutorial](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/scikit_learn_data_processing_and_model_evaluation.ipynb) to learn how to build a python script that Sagemaker will call to build the job.\n",
    "\n",
    "Make sure the script is versioned in Github. \n",
    "\n",
    "### Preprocessing steps:\n",
    "\n",
    "1. Create a `make_column_transformer` \n",
    "    - `make_pipeline` \n",
    "        - `RobustScaler`\n",
    "    - `make_pipeline` \n",
    "        - `OneHotEncoder` \n",
    "        \n",
    "The script is available in Github, [02_Data_analysis/01_model_training](https://github.com/Optimum-Finance/creditsafePrediction/blob/master/02_Data_analysis/01_model_training/preprocessing.py) and called by the `run` function from [ALGORITHM/PYTHON_SCRIPTS](https://s3.console.aws.amazon.com/s3/buckets/creditsafedata/ALGORITHM/PYTHON_SCRIPTS/?region=eu-west-2&tab=overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "#import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    # Prepare paths in Sagemaker instance\n",
    "    ## Input \n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'X_TRAIN_INVOICE_FINANCE.csv')\n",
    "    ## Output preprocessing job \n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_features.csv')\n",
    "    \n",
    "    #train_labels_output_path = os.path.join('/opt/ml/processing/train', 'train_labels.csv')\n",
    "    \n",
    "    # Open data\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    \n",
    "    X_train = df.drop(columns = ['status'])\n",
    "    #y_train = df[['status']]\n",
    "    \n",
    "    # Build preprocessing jobs\n",
    "    feat_obj = (X_train\n",
    "            .dtypes\n",
    "            .loc[lambda x : \n",
    "                 (x =='object') \n",
    "                &(x.index != 'row_id')\n",
    "                ]\n",
    "            .index\n",
    "            .to_list()\n",
    "           )\n",
    "    feat_cont = (X_train\n",
    "            .dtypes\n",
    "            .loc[lambda x : (x =='float64')  & (x.index != 'row_id')]\n",
    "            .index\n",
    "            .to_list()\n",
    "           )\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', RobustScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, feat_cont),\n",
    "            ('cat', categorical_transformer, feat_obj)]\n",
    "    )\n",
    "    \n",
    "    pip_preprocessor = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    \n",
    "    ### Need to convert label \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(df[['status']])\n",
    "    \n",
    "    # Call the job\n",
    "    train_features = pip_preprocessor.fit_transform(X_train)\n",
    "    \n",
    "    (pd.DataFrame(\n",
    "        np.column_stack((y_train,train_features))\n",
    "    ).to_csv(train_features_output_path,\n",
    "             header=False,\n",
    "             index=False)\n",
    "    )\n",
    "    \n",
    "    ### temporary solution\n",
    "    test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_features.csv')\n",
    "    test_label_output_path = os.path.join('/opt/ml/processing/test', 'test_labels.csv')\n",
    "    \n",
    "    # Save the data to Sagemaker\n",
    "    ### XGBoost requires label as the first columns\n",
    "    pd.DataFrame(train_features).to_csv(test_features_output_path,\n",
    "                                        header=False,\n",
    "                                        index=False)\n",
    "    \n",
    "    pd.DataFrame(y_train).to_csv(test_label_output_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\n",
    "    file_to_upload ='preprocessing.py',\n",
    "    destination_in_s3 = 'ALGORITHM/PYTHON_SCRIPTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = 's3://creditsafedata/ALGORITHM/{0}/DATA/TRAIN/RAW/{1}'.format(today, filename_x_train)\n",
    "destination_data_train = 's3://creditsafedata/ALGORITHM/{0}/DATA/TRAIN/TRANSFORM/'.format(today)\n",
    "destination_data_test = 's3://creditsafedata/ALGORITHM/{0}/DATA/EVALUATION/TRANSFORM/'.format(today)\n",
    "process_py = \"s3://creditsafedata/ALGORITHM/PYTHON_SCRIPTS/preprocessing.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor.run(code=process_py,\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=input_data,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train',\n",
    "                                               destination = destination_data_train),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test',\n",
    "                                               destination =destination_data_test)\n",
    "                              ]\n",
    "                     )\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "output_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "preprocessed_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluation.py\n",
    "\n",
    "import os\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#os.system('pip install joblib')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import xgboost\n",
    "#from sklearn.externals import joblib\n",
    "import json, tarfile, joblib, pickle\n",
    "\n",
    "def convert(o):\n",
    "        if isinstance(o, np.int64): return int(o)  \n",
    "        raise TypeError\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    model_path = os.path.join('/opt/ml/processing/model', 'model.tar.gz')\n",
    "    \n",
    "    print('Extracting model from path: {}'.format(model_path))\n",
    "    \n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path='.')\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "    \n",
    "    test_features_data = os.path.join('/opt/ml/processing/test', 'test_features.csv')\n",
    "    test_labels_data = os.path.join('/opt/ml/processing/test', 'test_labels.csv')\n",
    "                                    \n",
    "    X_test = xgboost.DMatrix('{}?format=csv'.format(test_features_data))\n",
    "    y_test = pd.read_csv(test_labels_data, header=None)\n",
    "    predictions = np.round(model.predict(X_test),0)\n",
    "        \n",
    "    conf_mat =confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    dic_metrics = {\n",
    "    \"score\": [\n",
    "        {\n",
    "            \"confusion_matrix\": dict(enumerate(conf_mat.flatten(), 1)),\n",
    "            \"classification_report\": classification_report(\n",
    "                y_test,\n",
    "                predictions,\n",
    "                target_names=[\"Not User\", \"User\"],\n",
    "                output_dict=True,\n",
    "            ),\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "    evaluation_output_path = os.path.join('/opt/ml/processing/evaluation', 'evaluation.json')\n",
    "    with open(evaluation_output_path, 'w') as f:\n",
    "        f.write(json.dumps(dic_metrics,default=convert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(\n",
    "    file_to_upload ='evaluation.py',\n",
    "    destination_in_s3 = 'ALGORITHM/PYTHON_SCRIPTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_py = \"s3://creditsafedata/ALGORITHM/PYTHON_SCRIPTS/evaluation.py\"\n",
    "destination_evaluation = 's3://creditsafedata/ALGORITHM/{}/EVALUATION/'.format(today)\n",
    "preprocessed_testing_data = 's3://creditsafedata/ALGORITHM/{}/DATA/EVALUATION/TRANSFORM/'.format(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sklearn_processor.run(code=evaluate_py,\n",
    "                      inputs=[ProcessingInput(\n",
    "                                  source=model_data_s3_uri,\n",
    "                                  destination='/opt/ml/processing/model'),\n",
    "                              ProcessingInput(\n",
    "                                  source=preprocessed_testing_data,\n",
    "                                  destination='/opt/ml/processing/test')],\n",
    "                      outputs=[ProcessingOutput(output_name='evaluation',\n",
    "                                  source='/opt/ml/processing/evaluation',\n",
    "                                               destination = destination_evaluation)]\n",
    "                     )                    \n",
    "evaluation_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now retrieve the file `evaluation.json` from Amazon S3, which contains the evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_output_config = evaluation_job_description['ProcessingOutputConfig']\n",
    "for output in evaluation_output_config['Outputs']:\n",
    "    if output['OutputName'] == 'evaluation':\n",
    "        evaluation_s3_uri = '{}evaluation.json'.format(output['S3Output']['S3Uri']) \n",
    "        break\n",
    "evaluation_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_output = S3Downloader.read_file(evaluation_s3_uri)\n",
    "evaluation_output_dict = json.loads(evaluation_output)\n",
    "print(json.dumps(evaluation_output_dict, sort_keys=True, indent=4))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.23.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
